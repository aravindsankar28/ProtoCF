{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bottleneck as bn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "import bottleneck as bn\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)  # indices of the top-k items (not in order).\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)  # [B, I]\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "    X_true_binary = (heldout_batch > 0)  # .toarray() #  [B, I]\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall\n",
    "\n",
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    \"\"\"\n",
    "    batch_users = X_pred.shape[0]  # batch_size\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk] * tp).sum(axis=1)\n",
    "\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in np.count_nonzero(heldout_batch, axis=1)])\n",
    "    return DCG / IDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prediction(fname):\n",
    "    \"\"\" Load model output\n",
    "        We assume predictions & targets (ground_truth) are aligned\n",
    "        Return - a list of model outputs\"\"\"\n",
    "    try:\n",
    "        preds = np.load(fname, allow_pickle=True)['preds']\n",
    "        if len(preds.shape) ==1:\n",
    "            return np.squeeze(np.concatenate(preds, axis=0))\n",
    "        else:\n",
    "            return np.squeeze(preds)\n",
    "    except:\n",
    "        try:\n",
    "            preds = np.load(fname, allow_pickle=True)['arr_0']\n",
    "            if len(preds.shape) ==1:\n",
    "                return np.squeeze(np.concatenate(preds, axis=0))\n",
    "            else:\n",
    "                return np.squeeze(preds)            \n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "\n",
    "def load_targets(dataset, mode='test', num_tr=5):\n",
    "    \"\"\" Load ground truth val/test user-item matrix along with user activity (# interacted items in\n",
    "    the fold-in set)\"\"\"\n",
    "    df_ui = pd.read_csv('~/few-shot-rec/data/{}/popular_train.csv'.format(dataset))  # read train set.\n",
    "    n_items = df_ui['item'].max() + 1\n",
    "    n_users = df_ui['user'].max() + 1\n",
    "    print (\"# unique users\", len(df_ui.user.unique()))\n",
    "    \n",
    "    all_user_activity = df_ui.groupby('user').size().reset_index(name='counts') #.values.astype(np.float32)\n",
    "        \n",
    "    print (\"n_users\", n_users)\n",
    "    \n",
    "    df_ui_tr_te = pd.read_csv('~/few-shot-rec/data/{}/{}.csv'.format(dataset, mode))\n",
    "    df_ui_tr_te = df_ui_tr_te[['item', 'user_0']]\n",
    "    df_ui_tr_te = df_ui_tr_te.rename({'user_0': 'user'}, axis=1)\n",
    "    \n",
    "    df_ui_eval_tr = df_ui_tr_te.groupby('item').apply(lambda x: x[:num_tr])\n",
    "    df_ui_eval_te = df_ui_tr_te[~df_ui_tr_te.index.isin(df_ui_eval_tr.index.get_level_values(1))]\n",
    "        \n",
    "    print (len(df_ui_eval_tr), len(df_ui_eval_te), len(df_ui_tr_te))\n",
    "    print (df_ui_eval_te.columns, df_ui_eval_tr.columns)\n",
    "\n",
    "    start_idx, end_idx = df_ui_eval_te['item'].min(), df_ui_eval_te['item'].max()\n",
    "    n_few_shot_items = end_idx - start_idx + 1\n",
    "    \n",
    "    rows_ui, cols_ui = df_ui_eval_te['user'], df_ui_eval_te['item'] - start_idx\n",
    "    rows_ui_tr, cols_ui_tr = df_ui_eval_tr['user'], df_ui_eval_tr['item'] - start_idx\n",
    "    \n",
    "    \n",
    "    \n",
    "    fold_in = sparse.csr_matrix((np.ones_like(rows_ui_tr), (rows_ui_tr, cols_ui_tr)), \n",
    "                                     dtype='float32', shape=(n_users, n_few_shot_items))\n",
    "    \n",
    "    # [# eval users x items] sparse matrix.\n",
    "    data_ui_test = sparse.csr_matrix((np.ones_like(rows_ui), (rows_ui, cols_ui)), \n",
    "                                     dtype='float32', shape=(n_users, n_few_shot_items))\n",
    "    \n",
    "    test_users = torch.arange(n_users)[~torch.all(torch.from_numpy(data_ui_test.toarray()) == 0, dim=1)].numpy()\n",
    "    print (\"# test_users\", len(test_users))\n",
    "        \n",
    "    # test_users = df_ui_eval_te['user'].unique()\n",
    "    n_test_users = len(test_users)\n",
    "    \n",
    "    test_users_activity = np.zeros(n_test_users)\n",
    "    for idx, u in enumerate(test_users):\n",
    "        try:\n",
    "            test_users_activity[idx]  = all_user_activity[all_user_activity['user'] == u]['counts'].values[0]\n",
    "        except:\n",
    "            test_users_activity[idx] = 0\n",
    "            \n",
    "    fold_in = fold_in[test_users, :]\n",
    "    data_ui_test = data_ui_test[test_users, :]  # n_test_users, n_few_shot_items\n",
    "    return fold_in.toarray(), data_ui_test.toarray(), test_users_activity\n",
    "\n",
    "def load_targets_all(dataset, num_tr=5, mode='popular'):\n",
    "    \"\"\" Load ground truth val/test user-item matrix along with user activity (# interacted items in\n",
    "    the fold-in set)\"\"\"\n",
    "    df_ui = pd.read_csv('~/few-shot-rec/data/{}/popular_train.csv'.format(dataset))  # read train set.\n",
    "    n_items = df_ui['item'].max() + 1\n",
    "    n_users = df_ui['user'].max() + 1\n",
    "    print (\"# unique users\", len(df_ui.user.unique()))\n",
    "    \n",
    "    all_user_activity = df_ui.groupby('user').size().reset_index(name='counts') #.values.astype(np.float32)\n",
    "        \n",
    "    print (\"n_users\", n_users)\n",
    "    if mode == \"popular\":\n",
    "        df_ui_eval_tr = df_ui\n",
    "        df_ui_tr_te = pd.read_csv('~/few-shot-rec/data/{}/popular_test.csv'.format(dataset, mode))\n",
    "        df_ui_eval_te = df_ui_tr_te\n",
    "        df_popular = pd.DataFrame()\n",
    "        df_popular = df_popular.append(df_ui_eval_tr)\n",
    "        df_popular = df_popular.append(df_ui_eval_te)\n",
    "        all_item_activity = df_popular.groupby('item').size().reset_index(name='counts')\n",
    "    else:\n",
    "        df_ui_tr_te = pd.read_csv('~/few-shot-rec/data/{}/test.csv'.format(dataset))\n",
    "        df_ui_tr_te = df_ui_tr_te[['item', 'user_0']]\n",
    "        df_ui_tr_te = df_ui_tr_te.rename({'user_0': 'user'}, axis=1)    \n",
    "#         df_val_tr_te = pd.read_csv('~/few-shot-rec/data/{}/val.csv'.format(dataset))\n",
    "#         df_val_tr_te = df_val_tr_te[['item', 'user_0']]\n",
    "#         df_val_tr_te = df_val_tr_te.rename({'user_0': 'user'}, axis=1)\n",
    "#         df_ui_tr_te = df_ui_tr_te.append(df_val_tr_te)\n",
    "        \n",
    "        all_item_activity = df_ui_tr_te.groupby('item').size().reset_index(name='counts')        \n",
    "        df_ui_eval_tr = df_ui_tr_te.groupby('item').apply(lambda x: x[:int(0.7 * len(x))])\n",
    "        # df_ui_eval_tr = df_ui_eval_tr.append(df_val_tr_te.groupby('item').apply(lambda x: x[:num_tr]))                \n",
    "        df_ui_eval_te = df_ui_tr_te[~df_ui_tr_te.index.isin(df_ui_eval_tr.index.get_level_values(1))]\n",
    "\n",
    "    print (len(df_ui_eval_tr), len(df_ui_eval_te), len(df_ui_tr_te))\n",
    "    print (df_ui_eval_te.columns, df_ui_eval_tr.columns)\n",
    "\n",
    "    start_idx, end_idx = df_ui_eval_te['item'].min(), df_ui_eval_te['item'].max()\n",
    "    n_few_shot_items = end_idx - start_idx + 1\n",
    "    \n",
    "    rows_ui, cols_ui = df_ui_eval_te['user'], df_ui_eval_te['item'] - start_idx\n",
    "    rows_ui_tr, cols_ui_tr = df_ui_eval_tr['user'], df_ui_eval_tr['item'] - start_idx\n",
    "    \n",
    "        \n",
    "    fold_in = sparse.csr_matrix((np.ones_like(rows_ui_tr), (rows_ui_tr, cols_ui_tr)), \n",
    "                                     dtype='float32', shape=(n_users, n_few_shot_items))\n",
    "    \n",
    "    # [# eval users x items] sparse matrix.\n",
    "    data_ui_test = sparse.csr_matrix((np.ones_like(rows_ui), (rows_ui, cols_ui)), \n",
    "                                     dtype='float32', shape=(n_users, n_few_shot_items))\n",
    "    \n",
    "    test_users = torch.arange(n_users)[~torch.all(torch.from_numpy(data_ui_test.toarray()) == 0, dim=1)].numpy()\n",
    "    print (\"# test_users\", len(test_users))\n",
    "        \n",
    "    # test_users = df_ui_eval_te['user'].unique()\n",
    "    n_test_users = len(test_users)\n",
    "    \n",
    "    items_activity = np.zeros(n_few_shot_items)\n",
    "    for idx, item in enumerate(range(start_idx, end_idx+1)):\n",
    "        try:\n",
    "            items_activity[idx]  = all_item_activity[all_item_activity['item'] == item]['counts'].values[0]\n",
    "        except:\n",
    "            items_activity[idx] = 0\n",
    "            \n",
    "    test_users_activity = np.zeros(n_test_users)\n",
    "    for idx, u in enumerate(test_users):\n",
    "        try:\n",
    "            test_users_activity[idx] = all_user_activity[all_user_activity['user'] == u]['counts'].values[0]\n",
    "        except:\n",
    "            test_users_activity[idx] = 0\n",
    "            \n",
    "    fold_in = fold_in[test_users, :]\n",
    "    data_ui_test = data_ui_test[test_users, :]  # n_test_users, n_few_shot_items\n",
    "    return fold_in.toarray(), data_ui_test.toarray(), test_users_activity, test_users, items_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_targets_joint(dataset):\n",
    "    \"\"\" Load ground truth val/test user-item matrix along with user activity (# interacted items in\n",
    "    the fold-in set)\"\"\"\n",
    "    df_popular_train = pd.read_csv('~/few-shot-rec/data/{}/popular_train.csv'.format(dataset))  # read train set.\n",
    "    n_popular = df_popular_train['item'].max() + 1\n",
    "    n_users = df_popular_train['user'].max() + 1\n",
    "    print (\"max users\", n_users)\n",
    "    print (\"# unique users\", len(df_popular_train.user.unique()))\n",
    "    \n",
    "    all_user_activity = df_popular_train.groupby('user').size().reset_index(name='counts') #.values.astype(np.float32)\n",
    "    \n",
    "    df_all = pd.DataFrame()\n",
    "    \n",
    "    df_popular_test = pd.read_csv('~/few-shot-rec/data/{}/popular_test.csv'.format(dataset))\n",
    "    df_popular_eval = pd.read_csv('~/few-shot-rec/data/{}/popular_val.csv'.format(dataset))\n",
    "    \n",
    "    df_popular = pd.DataFrame()\n",
    "    df_all = df_all.append(df_popular_train)\n",
    "    df_all = df_all.append(df_popular_test)\n",
    "        \n",
    "    df_fs_test = pd.read_csv('~/few-shot-rec/data/{}/test.csv'.format(dataset))\n",
    "    df_all = df_all.append(df_fs_test)\n",
    "    \n",
    "    df_fs_test = df_fs_test[['item', 'user_0']]\n",
    "    df_fs_test = df_fs_test.rename({'user_0': 'user'}, axis=1)  \n",
    "    \n",
    "    df_fs_train = df_fs_test.groupby('item').apply(lambda x: x[:int(0.7 * len(x))])\n",
    "    df_fs_test = df_fs_test[~df_fs_test.index.isin(df_fs_train.index.get_level_values(1))]\n",
    "    \n",
    "    ## GETTING FEWSHOT TARGET\n",
    "    start_idx, end_idx = df_fs_test['item'].min(), df_fs_test['item'].max()\n",
    "    n_few_shot_items = end_idx - start_idx + 1\n",
    "    \n",
    "    rows_ui, cols_ui = df_fs_test['user'], df_fs_test['item'] - start_idx\n",
    "    \n",
    "    # [# eval users x items] sparse matrix.\n",
    "    data_fs_test = sparse.csr_matrix((np.ones_like(rows_ui), (rows_ui, cols_ui)), \n",
    "                                     dtype='float32', shape=(n_users, n_few_shot_items))\n",
    "    \n",
    "    ## GETTING POPULAR TARGET\n",
    "    rows_te, cols_te = df_popular_test['user'], df_popular_test['item']\n",
    "    \n",
    "    ui_data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float32',\n",
    "                                   shape=(n_users, n_popular))\n",
    "    \n",
    "    all_target = sparse.hstack([ui_data_te, data_fs_test])    \n",
    "    all_item_activity = df_all.groupby('item').size().reset_index(name='counts')\n",
    "    items_activity = np.zeros(n_popular + n_few_shot_items)\n",
    "    \n",
    "    for idx, item in enumerate(range(0, n_popular+1)):\n",
    "        try:\n",
    "            items_activity[idx]  = all_item_activity[all_item_activity['item'] == item]['counts'].values[0]\n",
    "        except:\n",
    "            items_activity[idx] = 0\n",
    "    \n",
    "    \n",
    "    for idx, item in enumerate(range(start_idx, end_idx+1)):\n",
    "        try:\n",
    "            items_activity[n_popular+idx]  = all_item_activity[all_item_activity['item'] == item]['counts'].values[0]\n",
    "        except:\n",
    "            items_activity[n_popular+idx] = 0\n",
    "    test_users = np.arange(0, n_users)\n",
    "\n",
    "    if dataset == 'yelp_AZ0.3_processed_item':\n",
    "        test_users = np.array(all_user_activity[all_user_activity.counts > 5].user)\n",
    "        \n",
    "    if dataset == 'gowalla0.3_processed_item':\n",
    "        test_users = np.array(all_user_activity[all_user_activity.counts > 10].user)\n",
    "        \n",
    "    if dataset == 'weeplaces0.3_processed_item':\n",
    "        test_users = np.array(all_user_activity[all_user_activity.counts > 15].user)\n",
    "\n",
    "    print (\"# test users\", len(test_users))\n",
    "    # test_users = \n",
    "    \n",
    "#     test_users = torch.arange(n_users)[~torch.all(torch.from_numpy(data_ui_test.toarray()) == 0, dim=1)].numpy()\n",
    "#     print (\"# test_users\", len(test_users))\n",
    "        \n",
    "#     # test_users = df_ui_eval_te['user'].unique()\n",
    "#     n_test_users = len(test_users)\n",
    "    \n",
    "#     items_activity = np.zeros(n_few_shot_items)\n",
    "#     for idx, item in enumerate(range(start_idx, end_idx+1)):\n",
    "#         try:\n",
    "#             items_activity[idx]  = all_item_activity[all_item_activity['item'] == item]['counts'].values[0]\n",
    "#         except:\n",
    "#             items_activity[idx] = 0\n",
    "            \n",
    "#     test_users_activity = np.zeros(n_test_users)\n",
    "#     for idx, u in enumerate(test_users):\n",
    "#         try:\n",
    "#             test_users_activity[idx] = all_user_activity[all_user_activity['user'] == u]['counts'].values[0]\n",
    "#         except:\n",
    "#             test_users_activity[idx] = 0\n",
    "            \n",
    "#     fold_in = fold_in[test_users, :]\n",
    "#     data_ui_test = data_ui_test[test_users, :]  # n_test_users, n_few_shot_items\n",
    "    return all_target.toarray(), items_activity, test_users\n",
    "\n",
    "# all_target, items_activity, test_users = load_targets_joint('{}_processed_item'.format('gowalla0.3'))\n",
    "#print (\"all_target\", all_target.shape, \"items_activity\", items_activity.shape, \"test_users\", test_users.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #len(all_user_activity[all_user_activity.counts > 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch_torch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "    _, topk_indices = torch.topk(X_pred, k, dim=1, sorted=False)  # [B, K]\n",
    "    X_pred_binary = torch.zeros_like(X_pred)\n",
    "    if torch.cuda.is_available():\n",
    "        X_pred_binary = X_pred_binary.cuda()\n",
    "    X_pred_binary[torch.arange(batch_users).unsqueeze(1), topk_indices] = 1\n",
    "    X_true_binary = (heldout_batch > 0).float()  # .toarray() #  [B, I]\n",
    "    k_tensor = torch.Tensor([k])\n",
    "    if torch.cuda.is_available():\n",
    "        X_true_binary = X_true_binary.cuda()\n",
    "        k_tensor = k_tensor.cuda()\n",
    "    tmp = (X_true_binary * X_pred_binary).sum(dim=1).float()\n",
    "    recall = tmp / torch.min(k_tensor, X_true_binary.sum(dim=1).float())\n",
    "    return recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#all_target = torch.tensor(all_target)\n",
    "#vaepred = torch.tensor(load_prediction('/data/prediction/our_model/{}bpr_memory_few_shot_test_preds_new.npz'.format('weeplaces0.3')))\n",
    "#vaepred = torch.tensor(load_prediction('/data/prediction/our_model/{}vae_memory_few_shot_test_preds_new.npz'.format('weeplaces0.3')))\n",
    "\n",
    "#print(vaepred.shape)\n",
    "#unique_users = torch.arange(vaepred.shape[0])[~torch.all(all_target == 0, dim=1)].numpy()\n",
    "\n",
    "#print(torch.mean(Recall_at_k_batch_torch(vaepred[unique_users], all_target[unique_users], k=50)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pandas_explode(df, column_to_explode, cols):\n",
    "    # Create a list of new observations\n",
    "    new_observations = list()\n",
    "    # Iterate through existing observations\n",
    "    for row in df.to_dict(orient='records'):\n",
    "        # Take out the exploding iterable\n",
    "        explode_values = row[column_to_explode]\n",
    "        del row[column_to_explode]\n",
    "        # Create a new observation for every entry in the exploding iterable & add all of the other columns\n",
    "        \n",
    "        print(len(explode_values))\n",
    "        for idx, explode_value in enumerate(explode_values):\n",
    "            # Deep copy existing observation\n",
    "            new_observation = copy.deepcopy(row)\n",
    "            # Add one (newly flattened) value from exploding iterable\n",
    "            new_observation[column_to_explode] = explode_value\n",
    "            for c_name in cols:\n",
    "                print(c_name)\n",
    "                print(idx)\n",
    "                print(cols[c_name])\n",
    "                new_observation[c_name] = cols[c_name][idx]\n",
    "            # new_observation['group_size'] = g_sizes[idx]\n",
    "            # Add to the list of new observations\n",
    "            new_observations.append(new_observation)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    return_df = pd.DataFrame(new_observations)\n",
    "\n",
    "    # Return\n",
    "    return return_df\n",
    "\n",
    "\n",
    "\n",
    "def get_df(dataset, models_paths, metric, K, mode='test'):\n",
    "    df = pd.DataFrame(columns=['dataset', 'variant', 'metric', 'K', 'score'])\n",
    "    fold_in, targets, user_activity = load_targets(dataset, mode=mode, num_tr=5)\n",
    "    rows_list = []\n",
    "    if metric == 'NDCG':\n",
    "        metric_fn = NDCG_binary_at_k_batch\n",
    "    else:    \n",
    "        metric_fn = Recall_at_k_batch\n",
    "    \n",
    "    for model, path in models_paths.items():\n",
    "        preds = load_prediction(path)\n",
    "        if preds is None:\n",
    "            continue\n",
    "        if model == 'Ours':\n",
    "            preds = preds.transpose()            \n",
    "        print (\"preds\", preds.shape, \"targets\", targets.shape, \"activity\", user_activity.shape)\n",
    "        # preds[fold_in.nonzero()] = -np.inf\n",
    "        score = metric_fn(preds, targets, K)\n",
    "        row_dict = {'dataset': dataset, 'variant': model, 'metric': metric, 'K' : K, 'score': score}\n",
    "        rows_list.append(row_dict)\n",
    "    df = pd.DataFrame(rows_list)\n",
    "    df = pandas_explode(df, 'score', cols={'user_activity': user_activity})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df_all(dataset, model_paths, metric, K, mode='popular'):\n",
    "    df = pd.DataFrame(columns=['dataset', 'variant', 'metric', 'K', 'score'])\n",
    "    fold_in_popular, targets_popular, user_activity_popular,popular_test_user = load_targets_all(dataset, num_tr=5, mode='popular')\n",
    "    fold_in_fs, targets_fs, user_activity_fs, fs_test_user = load_targets_all(dataset, num_tr=5, mode=\"fewshot\")\n",
    "    rows_list_popular = []\n",
    "    rows_list_fs = []\n",
    "    if metric == 'NDCG':\n",
    "        metric_fn = NDCG_binary_at_k_batch\n",
    "    else:    \n",
    "        metric_fn = Recall_at_k_batch\n",
    "    for model, path in model_paths.items():\n",
    "        preds = load_prediction(path)\n",
    "        print(preds)\n",
    "        if preds is None:\n",
    "            continue\n",
    "        if model == 'Ours':\n",
    "            preds = preds.transpose()            \n",
    "        # print (\"preds\", preds.shape, \"targets\", targets.shape, \"activity\", user_activity.shape)\n",
    "        # preds[fold_in.nonzero()] = -np.inf\n",
    "        score_popular = metric_fn(preds[:, :targets_popular.shape[1]], targets_popular, K)\n",
    "        score_fs = metric_fn(preds[:, targets_fs.shape[1]:], targets_fs, K)\n",
    "        row_dict = {'dataset': dataset, 'variant': model, 'metric': metric, 'K' : K, 'score': score_popular}\n",
    "        row_dict2 = {'dataset': dataset, 'variant': model, 'metric': metric, 'K' : K, 'score': score_fs}\n",
    "        rows_list_popular.append(row_dict)\n",
    "        rows_list_fs.append(row_dict2)\n",
    "    df_popular = pd.DataFrame(rows_list_popular)\n",
    "    df_popular = pandas_explode(df_popular, 'score', cols={'user_activity': user_activity_popular})\n",
    "    df_fs = pd.DataFrame(rows_list_fs)\n",
    "    df_fs = pandas_explode(df_fs, 'score', cols={'user_activity': user_activity_fs})\n",
    "    return df_popular, df_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to create bins based on a specified value (say user activity).\n",
    "\n",
    "def cut(df, n_bins, key, range_min=None, range_max=None):\n",
    "    if range_min is not None:\n",
    "        assert range_max is not None\n",
    "        ranges = np.arange(range_min, range_max + 0.1, (range_max - range_min) / n_bins)\n",
    "        # ranges[-1] = float(df[key].max())\n",
    "        print(ranges)\n",
    "        bins = pd.cut(df[key], ranges)\n",
    "        # bins = pd.qcut(df[key], n_bins)#, duplicates='drop')\n",
    "    else:\n",
    "        bins = pd.cut(df[key], n_bins)\n",
    "        bins = pd.qcut(df[key], n_bins)#, duplicates='drop')\n",
    "    new_df = df.copy()\n",
    "    new_df['bin'] = bins\n",
    "    return new_df\n",
    "\n",
    "def bin_data(df, key, n_bins, metric, range_min=None, range_max=None):\n",
    "    if range_min is not None:\n",
    "        assert range_max is not None\n",
    "        all_data_binned = cut(df, n_bins, key, range_min, range_max)\n",
    "    else:\n",
    "        all_data_binned = cut(df, n_bins, key)\n",
    "    return all_data_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# dataset = 'gowalla0.5'\n",
    "#dataset = 'weeplaces'\n",
    "# dataset = 'weeplaces0.3'\n",
    "# dataset = 'gowalla0.3'\n",
    "# dataset = 'epinions0.3'\n",
    "dataset = 'yelp_AZ0.3'\n",
    "datasets = ['{}_processed_item'.format(dataset)]\n",
    "# Static best model output paths for each dataset.\n",
    "models_paths = {}\n",
    "models_paths['{}_processed_item'.format(dataset)] = {\n",
    "    'BPR' : '/data/prediction/BPR/{}_all_preds_new.npz'.format(dataset),\n",
    "    'VAE-CF' : \"/data/prediction/VAE-CF/{}_all_preds_new.npz\".format(dataset),\n",
    "    # 'DAE' : \"/data/prediction/DAE/{}_5_all_preds.npz\".format(dataset),\n",
    "    # 'DAE_new' : \"/data/prediction/DAE/{}_5_new_all_preds.npz\".format(dataset),\n",
    "    # 'NCF' : '/data/prediction/NCF/{}_5_all_preds.npz'.format(dataset)\n",
    "    'Ours_VAE': '/data/prediction/our_model/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    'Ours_BPR': '/data/prediction/our_model/{}bpr_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    \"Meta_Rec\": '/data/prediction/meta_learning/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset)\n",
    "}\n",
    "print(models_paths)\n",
    "\n",
    "# Define best model paths here for each dataset (has to be hardcoded).\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# for ds in datasets:\n",
    "#     print (ds)\n",
    "#     df_ds = get_df(ds, models_paths[ds],'Recall', 50)\n",
    "#     df = df.append(df_ds)\n",
    "\n",
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_recall_outputs(targets, preds, test_users, item_activity):\n",
    "    targets_expanded, preds_expanded, users_expanded, items_expanded, item_activity_expanded = [], [], [], [], []\n",
    "    for u, pred, target in zip(test_users, preds, targets):\n",
    "        items = target.nonzero()[0]\n",
    "        for item in items:\n",
    "            arr = np.zeros(target.shape[0])\n",
    "            arr[item] = 1\n",
    "            targets_expanded.append(arr)\n",
    "            # targets_expanded.append(target)\n",
    "            preds_expanded.append(pred)\n",
    "            users_expanded.append(u)\n",
    "            items_expanded.append(item)\n",
    "            item_activity_expanded.append(item_activity[item])\n",
    "    targets_expanded, preds_expanded, users_expanded, items_expanded, item_activity_expanded = \\\n",
    "    np.array(targets_expanded), np.array(preds_expanded), np.array(users_expanded), \\\n",
    "    np.array(items_expanded), np.array(item_activity_expanded)\n",
    "    \n",
    "    return targets_expanded, preds_expanded, users_expanded, items_expanded, item_activity_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_item_recall(dataset, model_paths, metric, K, mode='popular'):\n",
    "    # fold_in_popular, targets_popular, user_activity_popular,popular_test_user, item_activity_popular = load_targets_all(dataset, num_tr=5, mode='popular')\n",
    "            \n",
    "    # fold_in_fs, targets_fs, user_activity_fs, fs_test_user, item_activity_fs = load_targets_all(dataset, num_tr=5, mode=\"fewshot\")\n",
    "    \n",
    "    targets, item_activity, test_users = load_targets_joint(dataset)  # TODO: get item_activity\n",
    "    \n",
    "    # print (user_activity_popular.shape, item_activity_popular.shape)\n",
    "    # assuming that masking has already been done.\n",
    "    # df_popular = pd.DataFrame(columns=['dataset', 'variant', 'metric', 'K', 'score'])\n",
    "\n",
    "    # df_popular = pd.DataFrame()\n",
    "    # df_fs = pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    # rows_list_popular = []\n",
    "    # rows_list_fs = []\n",
    "    rows_list = []\n",
    "    for model, path in model_paths.items():\n",
    "        print (model, path)\n",
    "        preds = load_prediction(path)\n",
    "        print (\"preds\", preds.shape)\n",
    "        if preds is None:\n",
    "            continue\n",
    "        if model == 'Ours':\n",
    "            preds = preds.transpose()\n",
    "            \n",
    "        # preds_popular = preds[popular_test_user, :targets_popular.shape[1]]\n",
    "        # preds_fs = preds[fs_test_user, targets_popular.shape[1]:]\n",
    "        # print (\"preds_popular\", preds_popular.shape, \"targets_popular\", targets_popular.shape)\n",
    "        # print (\"preds_fs\", preds_fs.shape, \"targets_fs\", targets_fs.shape)\n",
    "        print (\"preds\", preds.shape, \"targets\", targets.shape)\n",
    "                        \n",
    "#         targets_expanded_popular, preds_expanded_popular, users_expanded_popular, items_expanded_popular, \\\n",
    "#         item_activity_expanded_popular = get_item_recall_outputs(targets_popular, preds_popular, popular_test_user, \n",
    "#                                                                 item_activity_popular)\n",
    "\n",
    "#         targets_expanded_fs, preds_expanded_fs, users_expanded_fs, items_expanded_fs, item_activity_expanded_fs = \\\n",
    "#         get_item_recall_outputs(targets_fs, preds_fs, fs_test_user, item_activity_fs)\n",
    "        if metric == 'NDCG':\n",
    "            metric_fn = NDCG_binary_at_k_batch\n",
    "        else:    \n",
    "            metric_fn = Recall_at_k_batch\n",
    "        \n",
    "        df_model = pd.DataFrame()\n",
    "        \n",
    "        n_test_users = len(test_users)\n",
    "        for user_batch in np.array_split(test_users, 500):\n",
    "            batch_targets_expanded, batch_preds_expanded, batch_users_expanded, batch_items_expanded, \\\n",
    "            batch_item_activity_expanded = get_item_recall_outputs(targets[user_batch, :], \n",
    "                                                                   preds[user_batch, :], \n",
    "                                                                   user_batch, item_activity)            \n",
    "            \n",
    "#             targets_expanded, preds_expanded, users_expanded, items_expanded, item_activity_expanded = \\\n",
    "#             get_item_recall_outputs(targets, preds, test_users, item_activity)   \n",
    "        \n",
    "\n",
    "            row_dict = {'variant': model, 'user': batch_users_expanded, 'item': batch_items_expanded, \n",
    "                           'item_activity': batch_item_activity_expanded,\n",
    "                           'metric': metric, 'K' : K, \n",
    "                            'score': metric_fn(batch_preds_expanded, batch_targets_expanded, K)}\n",
    "        \n",
    "#             row_dict = {'variant': model, 'user': users_expanded, 'item': items_expanded, \n",
    "#                            'item_activity': item_activity_expanded,\n",
    "#                            'metric': metric, 'K' : K, \n",
    "#                             'score': metric_fn(preds_expanded, targets_expanded, K)}\n",
    "\n",
    "            df_model = df_model.append(pd.DataFrame(row_dict))\n",
    "        \n",
    "        # df_model = pd.DataFrame(row_dict).groupby(['variant', 'item', 'metric', 'K']).mean().reset_index()\n",
    "        df_model = df_model.groupby(['variant', 'item', 'metric', 'K']).mean().reset_index()\n",
    "        df = df.append(df_model)\n",
    "        \n",
    "#         row_dict_popular = {'variant': model,  'user': users_expanded_popular, 'item': items_expanded_popular, \n",
    "#                             'item_activity': item_activity_expanded_popular,\n",
    "#                             'metric': metric, 'K' : K, \n",
    "#                             'score': metric_fn(preds_expanded_popular, targets_expanded_popular, K)}\n",
    "        \n",
    "#         row_dict_fs = {'variant': model, 'user': users_expanded_fs, 'item': items_expanded_fs, \n",
    "#                        'item_activity': item_activity_expanded_fs,\n",
    "#                        'metric': metric, 'K' : K, \n",
    "#                         'score': metric_fn(preds_expanded_fs, targets_expanded_fs, K)}\n",
    "                        \n",
    "#         df_popular_model = pd.DataFrame(row_dict_popular).groupby(['variant', 'item', 'metric', 'K']).mean().reset_index()\n",
    "#         df_fs_model = pd.DataFrame(row_dict_fs).groupby(['variant', 'item', 'metric', 'K']).mean().reset_index()\n",
    "                                \n",
    "#         df_popular = df_popular.append(df_popular_model)\n",
    "#         df_fs = df_fs.append(df_fs_model)\n",
    "        \n",
    "\n",
    "    return df\n",
    "#models_paths\n",
    "dataset = datasets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp = get_df_item_recall(dataset, models_paths[dataset], 'Recall', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'weeplaces0.3'\n",
    "models_paths = {}\n",
    "models_paths['{}_processed_item'.format(dataset)] = {\n",
    "    'BPR' : '/data/prediction/BPR/{}_all_preds_new.npz'.format(dataset),\n",
    "    'VAE-CF' : \"/data/prediction/VAE-CF/{}_all_preds_new.npz\".format(dataset),\n",
    "    'Ours_VAE': '/data/prediction/our_model/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    'Ours_BPR': '/data/prediction/our_model/{}bpr_memory_few_shot_test_preds_new.npz'.format(dataset)\n",
    "}\n",
    "df_weeplaces = get_df_item_recall('weeplaces0.3_processed_item', models_paths['weeplaces0.3_processed_item'], 'Recall', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins= 10\n",
    "df_weeplaces['dataset'] = 'weeplaces'\n",
    "\n",
    "data_binned = pd.DataFrame()\n",
    "ds = df_weeplaces[(df_weeplaces.dataset == 'weeplaces') & (df_weeplaces.item_activity > 0)  ]\n",
    "\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score')\n",
    "data = binned_ds\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "# data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "data\n",
    "#data_binned['index'] = 10 - data_binned['index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'gowalla0.3'\n",
    "models_paths = {}\n",
    "models_paths['{}_processed_item'.format(dataset)] = {\n",
    "    'nodis': '/data/prediction/our_model/{}vae_nolambda_few_shot_test_preds_new_new.npz'.format(dataset),\n",
    "    'BPR' : '/data/prediction/BPR/{}_all_preds_new.npz'.format(dataset),\n",
    "    'VAE-CF' : \"/data/prediction/VAE-CF/{}_all_preds_new.npz\".format(dataset),\n",
    "    'Ours_VAE': '/data/prediction/our_model/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    'Ours_BPR': '/data/prediction/our_model/{}bpr_memory_few_shot_test_preds_new.npz'.format(dataset)\n",
    "    \n",
    "}\n",
    "df_gowalla = get_df_item_recall('gowalla0.3_processed_item', models_paths['gowalla0.3_processed_item'], 'Recall', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'epinions0.3'\n",
    "models_paths = {}\n",
    "models_paths['{}_processed_item'.format(dataset)] = {\n",
    "    'Pronet': '/data/prediction/our_model/{}vae_pronet_few_shot_test_preds_new_new.npz'.format(dataset), \n",
    "    'BPR' : '/data/prediction/BPR/{}_all_preds_new.npz'.format(dataset),\n",
    "    'VAE-CF' : \"/data/prediction/VAE-CF/{}_all_preds_new.npz\".format(dataset),\n",
    "    'Ours_VAE': '/data/prediction/our_model/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    'Ours_BPR': '/data/prediction/our_model/{}bpr_memory_few_shot_test_preds_new.npz'.format(dataset),\n",
    "    \"Meta_Rec\": '/data/prediction/meta_learning/{}vae_memory_few_shot_test_preds_new.npz'.format(dataset)\n",
    "    \n",
    "}\n",
    "df_epinions = get_df_item_recall('epinions0.3_processed_item', models_paths['epinions0.3_processed_item'], 'Recall', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_epinions['dataset'] = 'epinions'\n",
    "# df_gowalla['dataset'] = 'gowalla'\n",
    "df = pd.DataFrame()\n",
    "df_item = pd.DataFrame()\n",
    "# df = df.append(df_epinions)\n",
    "# df = df.append(df_gowalla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins= 2\n",
    "df_epinions['dataset'] = 'epinions'\n",
    "\n",
    "data_binned = pd.DataFrame()\n",
    "ds = df_epinions[(df_epinions.dataset == 'epinions') & (df_epinions.item_activity > 0) & (df_epinions.item_activity < 44)]\n",
    "ds.item_activity *= 0.7\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score', range_min=5, range_max=35)\n",
    "data = binned_ds\n",
    "print(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "#df = df.append(data_binned.groupby(['variant', 'dataset','bin']).mean()[['variant', 'dataset', 'score', 'index']])\n",
    "df = df.append(data_binned)\n",
    "#data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.DataFrame(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = items.reset_index()['bin'].reset_index()\n",
    "items = items.merge(idx, on='bin', how='inner')\n",
    "items['dataset']='epinions'\n",
    "items['variant']= 'cdf'\n",
    "for i in range(1, 5):\n",
    "    items['item'][i] += items['item'][i - 1]\n",
    "items['item'] *= 100/9035\n",
    "df_item = df_item.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins= 2\n",
    "df_yelp['dataset'] = 'yelp'\n",
    "\n",
    "data_binned = pd.DataFrame()\n",
    "ds = df_yelp[(df_yelp.dataset == 'yelp') & (df_yelp.item_activity > 10)& (df_yelp.item_activity < 44)]\n",
    "ds.item_activity *= 0.7\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score', range_min=5, range_max=35)\n",
    "data = binned_ds\n",
    "print(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "\n",
    "#data_binned['index'] = 10 - data_binned['index']\n",
    "df = df.append(data_binned)\n",
    "#df = df.append(data_binned.groupby(['variant', 'dataset','bin']).mean()[['variant', 'dataset', 'score', 'index']])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.DataFrame(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = items.reset_index()['bin'].reset_index()\n",
    "items = items.merge(idx, on='bin', how='inner')\n",
    "items['dataset']='yelp'\n",
    "items['variant']= 'cdf'\n",
    "for i in range(1, 5):\n",
    "    items['item'][i] += items['item'][i - 1]\n",
    "items['item'] *= 100/10451\n",
    "df_item = df_item.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins= 2\n",
    "df_weeplaces['dataset'] = 'weeplaces'\n",
    "data_binned = pd.DataFrame()\n",
    "ds = df_weeplaces[(df_weeplaces.dataset == 'weeplaces') & (df_weeplaces.item_activity > 10) & (df_weeplaces.item_activity < 44)]\n",
    "ds.item_activity *= 0.7\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score', range_min=5, range_max=35)\n",
    "data = binned_ds\n",
    "print(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "\n",
    "#df = df.append(data_binned.groupby(['variant', 'dataset','bin']).mean())\n",
    "df = df.append(data_binned)\n",
    "data\n",
    "#data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "#data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.DataFrame(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = items.reset_index()['bin'].reset_index()\n",
    "items = items.merge(idx, on='bin', how='inner')\n",
    "items['dataset']='weeplaces'\n",
    "items['variant']= 'cdf'\n",
    "for i in range(1, 5):\n",
    "    items['item'][i] += items['item'][i - 1]\n",
    "items['item'] *= 100/11679\n",
    "df_item = df_item.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_bins= 2\n",
    "df_gowalla['dataset'] = 'gowalla'\n",
    "data_binned = pd.DataFrame()\n",
    "ds = df_gowalla[(df_gowalla.dataset == 'gowalla') & (df_gowalla.item_activity > 10) & (df_gowalla.item_activity < 44)]\n",
    "\n",
    "ds.item_activity *= 0.7\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score', range_min=5, range_max=35)\n",
    "data = binned_ds\n",
    "print(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "#df = df.append(data_binned.groupby(['variant', 'dataset','bin']).mean())\n",
    "df = df.append(data_binned)\n",
    "\n",
    "#data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "#data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items = pd.DataFrame(binned_ds.groupby('bin')['item'].nunique())\n",
    "idx = items.reset_index()['bin'].reset_index()\n",
    "items = items.merge(idx, on='bin', how='inner')\n",
    "items['dataset']='gowalla'\n",
    "items['variant']= 'cdf'\n",
    "for i in range(1, 5):\n",
    "    items['item'][i] += items['item'][i - 1]\n",
    "items['item'] *= 100/27920\n",
    "print(items)\n",
    "df_item = df_item.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binned['index'] = 5 - data_binned['index']\n",
    "\n",
    "intro_df = data_binned\n",
    "sns.set(rc={'figure.figsize':(11,6)})\n",
    "sns.set(font_scale=3)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "\n",
    "plot_model_names = ['BPR', 'Ours_BPR', 'VAE-CF', 'Ours_VAE']\n",
    "data_plot = intro_df[intro_df.variant.isin(plot_model_names)]\n",
    "\n",
    "pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "pal.reverse()\n",
    "pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x='index',y='score',hue='variant',data=data_plot, palette=pal,\n",
    "                hue_order=plot_model_names, linewidth=3, dashes = False)\n",
    "\n",
    "\n",
    "fontsize=18\n",
    "\n",
    "ax.set_xlabel(\"Item groups ordered by decreasing popularity \", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Item Recall$@50$\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # set subplot title\n",
    "\n",
    "ax.set_title(\"\", fontsize=fontsize)\n",
    "#ax.set_yticks(np.arange(0.15, 0.35, 0.1))\n",
    "# ax.set_yticks(np.arange(0.0, 0.41,  0.01), minor=True)\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(1, 6))\n",
    "\n",
    "ax.tick_params(direction='in', length=7, width=3, colors='k', which='major', labelsize=15, axis='y')\n",
    "ax.tick_params(direction='in', length=4, width=2, colors='gray', which='minor', axis='y')\n",
    "\n",
    "ax.tick_params(direction='in', length=7, width=3, colors='k', which='major', labelsize=15, axis='x')\n",
    "ax.tick_params(direction='in', length=4, width=2, colors='gray', which='minor', axis='x')\n",
    "\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "print(handles)\n",
    "#ax.legend(handles=handles[1:], labels=labels[1:], loc='upper center', bbox_to_anchor=(1, 1), ncol=2, \n",
    "#             frameon=False, fontsize=10)\n",
    "ax.legend(handles=handles[1:], labels=labels[1:],  \n",
    "             frameon=False, fontsize=fontsize)\n",
    "\n",
    "sns.despine(offset=20, left=True, trim=True)\n",
    "# plt.savefig('intro.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dataset.unique()\n",
    "name_map = {'Ours_BPR': 'ProtoCF + BPR', 'Ours_VAE': 'ProtoCF + VAE', 'BPR': 'BPR', 'VAE-CF': 'VAE-CF'}\n",
    "\n",
    "df['variant'] = df['variant'].apply(lambda x: name_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.variant.unique(), df.dataset.unique()\n",
    "\n",
    "# plot_bins = ['(5, 10]', '(10, 15]', \"(15, 20]\", \"(20, 25]\", '(25, 30]']\n",
    "plot_bins = ['5-10', '11-15', \"16-20\", \"21-25\", '26-30']\n",
    "\n",
    "# frequency_bins = [[11, 12, 14, 16, 19, 24, 28, 38, 60, 805],\n",
    "#                  [12, 15, 19, 24,  32, 40, 50, 68, 108, 759],\n",
    "#                  [18, 21, 25, 31, 34, 40, 49, 64, 95, 1480],\n",
    "#                  [12, 14, 16, 19, 24, 28, 33, 42, 66, 2124]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"ticks\")\n",
    "barWidth = 0.25\n",
    "# df['index'] = df['index'] + 1\n",
    "# df_item['index'] = df_item['index'] + 1\n",
    "plot_model_names = ['VAE-CF','BPR','ProtoCF + BPR','ProtoCF + VAE']\n",
    "\n",
    "# pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "# print(sns.color_palette(\"Blues\"))\n",
    "# pal.reverse()\n",
    "# pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "# pal[0] = sns.color_palette(\"Blues\")[-1]\n",
    "# pal[1] = pal[-2]\n",
    "\n",
    "pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "pal.reverse()\n",
    "pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "\n",
    "fig, ax = sns.mpl.pyplot.subplots(1, 4, sharey=False, figsize=(36, 6))\n",
    "\n",
    "fig.subplots_adjust(wspace=0.05, hspace=5)\n",
    "\n",
    "ax2 = ax[0].twinx()\n",
    "# ax2= sns.lineplot(data=df_item[df_item.dataset == 'epinions'], x = 'index', y='item')\n",
    "g2 = ax2.bar(np.arange(5) , df_item[df_item.dataset == 'epinions'].item, color='lightblue', alpha=0.5,\n",
    "             width=barWidth, edgecolor='white')\n",
    "\n",
    "\n",
    "ax2.set_yticks(np.arange(0, 101, 50))\n",
    "ax2.set_yticks(np.arange(0, 101, 10), minor=True)\n",
    "ax2.set_yticklabels(map(lambda x : x + \"%\", map(str, np.arange(0, 101, 50))))\n",
    "\n",
    "ax[0] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"epinions\"], \n",
    "                  palette=pal, ax = ax[0], hue_order=plot_model_names, size_order=plot_model_names,\n",
    "                     style_order=plot_model_names, \n",
    "                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "\n",
    "ax[1] = sns.lineplot(x='index',y='score',hue='variant', style='variant',  size='variant', data=df[df.dataset==\"yelp\"], \n",
    "                  palette=pal, ax = ax[1], hue_order=plot_model_names, style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False, \n",
    "                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "\n",
    "\n",
    "ax[2] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"weeplaces\"], \n",
    "                  palette=pal,  ax = ax[2], hue_order=plot_model_names,style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False,                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "ax[3] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"gowalla\"], \n",
    "                  palette=pal, ax = ax[3], hue_order=plot_model_names, style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False,                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "fontsize=28\n",
    "\n",
    "\n",
    "ax[0].set_yticks(np.arange(0.05, 0.201, 0.05))\n",
    "ax[0].set_yticks(np.arange(0.0, 0.201, 0.01), minor=True)\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_yticks(np.arange(0.05, 0.151, 0.05))\n",
    "ax[1].set_yticks(np.arange(0.0, 0.151, 0.01), minor=True)\n",
    "ax4 = ax[1].twinx()\n",
    "# ax4= sns.lineplot(data=df_item[df_item.dataset == 'yelp'], x = 'index', y='item\n",
    "g2 = ax4.bar(np.arange(5) , df_item[df_item.dataset == 'yelp'].item, color='lightblue', alpha=0.5,\n",
    "             width=barWidth, edgecolor='white')\n",
    "ax4.set_yticks(np.arange(0, 101, 50))\n",
    "ax4.set_yticks(np.arange(0, 101, 10), minor=True)\n",
    "ax4.set_yticklabels(map(lambda x : x + \"%\", map(str, np.arange(0, 101, 50))))\n",
    "ax[2].set_yticks(np.arange(0.05, 0.301, 0.05))\n",
    "ax[2].set_yticks(np.arange(0.05, 0.301, 0.01), minor=True)\n",
    "ax6 = ax[2].twinx()\n",
    "\n",
    "# ax6= sns.lineplot(data=df_item[df_item.dataset == 'weeplaces'], x = 'index', y='item')\n",
    "g2 = ax6.bar(np.arange(5) , df_item[df_item.dataset == 'weeplaces'].item, color='lightblue', alpha=0.5,\n",
    "             width=barWidth, edgecolor='white')\n",
    "ax6.set_yticks(np.arange(0, 101, 50))\n",
    "ax6.set_yticks(np.arange(0, 101, 10), minor=True)\n",
    "ax6.set_yticklabels(map(lambda x : x + \"%\", map(str, np.arange(0, 101, 50))))\n",
    "ax[3].set_yticks(np.arange(0.1, 0.251, 0.05))\n",
    "ax[3].set_yticks(np.arange(0.1, 0.251, 0.01), minor=True)\n",
    "ax8 = ax[3].twinx()\n",
    "# ax8= sns.lineplot(data=df_item[df_item.dataset == 'gowalla'], x = 'index', y='item')\n",
    "g2 = ax8.bar(np.arange(5), df_item[df_item.dataset == 'gowalla'].item, color='lightblue', alpha=0.5,\n",
    "             width=barWidth, edgecolor='white')\n",
    "ax8.set_yticks(np.arange(0, 101, 50))\n",
    "ax8.set_yticks(np.arange(0, 101, 10), minor=True)\n",
    "ax8.set_yticklabels(map(lambda x : x + \"%\", map(str, np.arange(0, 101, 50))))\n",
    "#ax[0].set_ylim([0.05, 0.201])\n",
    "#ax[1].set_ylim([0.05, 0.201])\n",
    "\n",
    "\n",
    "for idx in range(0, 4):\n",
    "    ax[idx].set_xlabel(\"Training interactions per item\", fontsize=fontsize)\n",
    "    #ax[idx].set_ylabel(\"Recall$@K$\", fontsize=fontsize)\n",
    "    ax[idx].set_ylabel('')\n",
    "ax[0].set_ylabel(\"Item Recall$@50$\", fontsize=fontsize)\n",
    "\n",
    "dataset_list = ['Epinions', 'Yelp', 'Weeplaces', 'Gowalla']\n",
    "for it in range(0, 4):\n",
    "    \n",
    "    ax[it].set_xticks(np.arange(0, 5))\n",
    "    ax[it].set_xticklabels(plot_bins)\n",
    "\n",
    "    ax[it].tick_params(direction='in', length=6, width=2, colors='k', which='major', axis='both')\n",
    "    ax[it].tick_params(direction='in', length=4, width=1, colors='gray', which='minor', axis='both')\n",
    "    \n",
    "    ax[it].tick_params(labelsize=20)\n",
    "    ax[it].legend()\n",
    "    ax[it].set_title(dataset_list[it])\n",
    "\n",
    "sns.despine(offset=10, left=True, trim=True)\n",
    "sns.mpl.pyplot.tight_layout()\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "print(handles)\n",
    "ax[0].legend(handles=handles[1:], labels=labels[1:], loc='upper center', bbox_to_anchor=(2.2, 1.35), ncol=7, frameon=False, fontsize=30)\n",
    "\n",
    "ax[1].legend(frameon=False)\n",
    "ax[2].legend(frameon=False)\n",
    "ax[3].legend(frameon=False)\n",
    "\n",
    "ax8.set_ylabel(\"Cumulative fraction of items\", fontsize=fontsize)\n",
    "\n",
    "plt.savefig('item_recall_counts.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bins[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# df['index'] = 10 - df['index']\n",
    "df['index'] = 5 - df['index']\n",
    "\n",
    "plot_model_names = ['VAE-CF','BPR','ProtoCF + BPR','ProtoCF + VAE']\n",
    "\n",
    "# pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "# print(sns.color_palette(\"Blues\"))\n",
    "# pal.reverse()\n",
    "# pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "# pal[0] = sns.color_palette(\"Blues\")[-1]\n",
    "# pal[1] = pal[-2]\n",
    "\n",
    "pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "pal.reverse()\n",
    "pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "\n",
    "fig, ax = sns.mpl.pyplot.subplots(1, 4, sharey=False, figsize=(36, 6))\n",
    "\n",
    "fig.subplots_adjust(wspace=0.05, hspace=5)\n",
    "\n",
    "ax[0] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"epinions\"], \n",
    "                  palette=pal, ax = ax[0], hue_order=plot_model_names, size_order=plot_model_names,\n",
    "                     style_order=plot_model_names, \n",
    "                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "\n",
    "ax[1] = sns.lineplot(x='index',y='score',hue='variant', style='variant',  size='variant', data=df[df.dataset==\"yelp\"], \n",
    "                  palette=pal, ax = ax[1], hue_order=plot_model_names, style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False, \n",
    "                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "ax[2] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"weeplaces\"], \n",
    "                  palette=pal,  ax = ax[2], hue_order=plot_model_names,style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False,                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "ax[3] = sns.lineplot(x='index',y='score',hue='variant', style='variant', size='variant', data=df[df.dataset==\"gowalla\"], \n",
    "                  palette=pal, ax = ax[3], hue_order=plot_model_names, style_order=plot_model_names, size_order=plot_model_names,\n",
    "             legend=False,                     dashes = [ (4, 1.5), (4, 1.5), (1, 0) ,  (1, 0),  \"\"],\n",
    "                    sizes=[1.5,1.5,2.5,2.5])\n",
    "\n",
    "fontsize=25\n",
    "\n",
    "\n",
    "ax[0].set_yticks(np.arange(0.05, 0.301, 0.05))\n",
    "ax[0].set_yticks(np.arange(0.0, 0.301, 0.01), minor=True)\n",
    "\n",
    "ax[1].set_yticks(np.arange(0.05, 0.301, 0.05))\n",
    "ax[1].set_yticks(np.arange(0.0, 0.301, 0.01), minor=True)\n",
    "\n",
    "ax[2].set_yticks(np.arange(0.05, 0.301, 0.05))\n",
    "ax[2].set_yticks(np.arange(0.05, 0.301, 0.01), minor=True)\n",
    "\n",
    "ax[3].set_yticks(np.arange(0.1, 0.301, 0.05))\n",
    "ax[3].set_yticks(np.arange(0.1, 0.301, 0.01), minor=True)\n",
    "\n",
    "#ax[0].set_ylim([0.05, 0.201])\n",
    "#ax[1].set_ylim([0.05, 0.201])\n",
    "\n",
    "\n",
    "for idx in range(0, 4):\n",
    "    ax[idx].set_xlabel(\"Item groups by decreasing frequency\", fontsize=fontsize)\n",
    "    #ax[idx].set_ylabel(\"Recall$@K$\", fontsize=fontsize)\n",
    "    ax[idx].set_ylabel('')\n",
    "ax[0].set_ylabel(\"Item Recall$@50$\", fontsize=fontsize)\n",
    "\n",
    "dataset_list = ['Epinions', 'Yelp', 'Weeplaces', 'Gowalla']\n",
    "for it in range(0, 4):\n",
    "    #ax[it].set_xticks(np.arange(1, 12, 2))\n",
    "    #ax[it].set_xticks(np.arange(1, 11, 1), minor=True)\n",
    "    # ax[it].set_xlim((1,11))\n",
    "\n",
    "    ax[it].set_xticks(np.arange(1, 11))    \n",
    "    # ax[it].set_xticklabels(list(map(str, range(5, 30, 5))))\n",
    "    ax[it].tick_params(direction='in', length=6, width=2, colors='k', which='major', axis='both')\n",
    "    ax[it].tick_params(direction='in', length=4, width=1, colors='gray', which='minor', axis='both')\n",
    "    \n",
    "    ax[it].tick_params(labelsize=15)\n",
    "    ax[it].legend()\n",
    "    ax[it].set_title(dataset_list[it])\n",
    "\n",
    "sns.despine(offset=10, left=True, trim=True)\n",
    "sns.mpl.pyplot.tight_layout()\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "print(handles)\n",
    "ax[0].legend(handles=handles[1:], labels=labels[1:], loc='upper center', bbox_to_anchor=(2.2, 1.35), ncol=7, frameon=False, fontsize=30)\n",
    "\n",
    "ax[1].legend(frameon=False)\n",
    "ax[2].legend(frameon=False)\n",
    "ax[3].legend(frameon=False)\n",
    "#plt.savefig('item_recall.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data_binned['index'] = 10 - data_binned['index']\n",
    "\n",
    "intro_df = data_binned\n",
    "sns.set(rc={'figure.figsize':(11,6)})\n",
    "sns.set(font_scale=3)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "\n",
    "plot_model_names = ['BPR', 'Ours_BPR', 'VAE-CF', 'Ours_VAE']\n",
    "data_plot = intro_df[intro_df.variant.isin(plot_model_names)]\n",
    "\n",
    "pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "pal.reverse()\n",
    "pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x='index',y='score',hue='variant',data=data_plot, palette=pal,\n",
    "                hue_order=plot_model_names, linewidth=3, dashes = False)\n",
    "\n",
    "\n",
    "fontsize=18\n",
    "\n",
    "ax.set_xlabel(\"Item groups ordered by decreasing popularity \", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Item Recall$@50$\", fontsize=fontsize)\n",
    "\n",
    "\n",
    "# # set subplot title\n",
    "\n",
    "ax.set_title(\"\", fontsize=fontsize)\n",
    "#ax.set_yticks(np.arange(0.15, 0.35, 0.1))\n",
    "# ax.set_yticks(np.arange(0.0, 0.41,  0.01), minor=True)\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(1, 11))\n",
    "\n",
    "ax.tick_params(direction='in', length=7, width=3, colors='k', which='major', labelsize=15, axis='y')\n",
    "ax.tick_params(direction='in', length=4, width=2, colors='gray', which='minor', axis='y')\n",
    "\n",
    "ax.tick_params(direction='in', length=7, width=3, colors='k', which='major', labelsize=15, axis='x')\n",
    "ax.tick_params(direction='in', length=4, width=2, colors='gray', which='minor', axis='x')\n",
    "\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "print(handles)\n",
    "#ax.legend(handles=handles[1:], labels=labels[1:], loc='upper center', bbox_to_anchor=(1, 1), ncol=2, \n",
    "#             frameon=False, fontsize=10)\n",
    "ax.legend(handles=handles[1:], labels=labels[1:],  \n",
    "             frameon=False, fontsize=fontsize)\n",
    "\n",
    "sns.despine(offset=20, left=True, trim=True)\n",
    "# plt.savefig('intro.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_popular, df_fs = get_df_item_recall(dataset, models_paths[dataset], 'Recall', 50 , mode='popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_popular.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_bins= 10\n",
    "df_fs['dataset'] = dataset\n",
    "df_popular['dataset'] = dataset\n",
    "\n",
    "data_binned = pd.DataFrame()\n",
    "    \n",
    "# ds = df_fs[(df_fs.dataset == dataset) ]\n",
    "ds = df_popular[(df_popular.dataset == dataset) ]\n",
    "# ds = ds.append(df_popular[(df_popular.dataset == dataset) ])\n",
    "\n",
    "binned_ds = bin_data(ds, 'item_activity', n_bins, 'score')\n",
    "data = binned_ds\n",
    "idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "data = data_binned.groupby(['variant', 'dataset','bin']).mean().score\n",
    "data_binned.to_pickle(\"dataframe/intro.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_BPR = data[:10].to_list()\n",
    "our_VAE = data[10:20].to_list()\n",
    "VAE = data[20:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1,2,3,4,5,6,7,8,9,10][::-1]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(bins, our_BPR, label=\"our_BPR\")\n",
    "plt.plot(bins, our_VAE, label=\"our_VAE\")\n",
    "plt.plot(bins, VAE, label=\"VAE\")\n",
    "# plt.plot(bins, DAE, label=\"DAE\")\n",
    "plt.xlabel(\"Item Activity from Most Active to Least\")\n",
    "plt.ylabel(\"Recall 50\")\n",
    "plt.legend()\n",
    "# dict_1 = {'dataset':'epinions0.5', 'variant': 'VAE', 'bins':bins, 'scores': VAE}\n",
    "# dict_2 = {'dataset':'epinions0.5', 'variant': 'DAE', 'bins':bins, 'scores': DAE}\n",
    "# temp_df = pd.DataFrame([dict_1, dict_2])\n",
    "# print(temp_df)\n",
    "# df_plot = pandas_explode(temp_df, 'bins', cols={'scores':temp_df['scores']})\n",
    "# df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEWSHOT\")\n",
    "print(df_fs[df_fs.variant == 'VAE-CF'].mean())\n",
    "print(df_fs[df_fs.variant == 'BPR'].mean())\n",
    "print(\"POPULAR\")\n",
    "print(df_popular[df_popular.variant == 'VAE-CF'].mean())\n",
    "print(df_popular[df_popular.variant == 'BPR'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dataset = 'gowalla'\n",
    "datasets = ['{}_processed_item'.format(dataset)]\n",
    "\n",
    "# Static best model output paths for each dataset.\n",
    "\n",
    "models_paths = {}\n",
    "models_paths['{}_processed_item'.format(dataset)] = {\n",
    "    'BPR' : '/data/prediction/BPR/{}_all_preds.npz'.format(dataset),\n",
    "    'VAE-CF' : \"/data/prediction/VAE-CF/{}_all_preds.npz\".format(dataset),\n",
    "    'DAE' : \"/data/prediction/DAE/{}_all_preds.npz\".format(dataset)\n",
    "    # 'Ours': '/data/prediction/our_model/{}_memory_few_shot_test_preds.npz'.format(dataset)\n",
    "}\n",
    "\n",
    "# Define best model paths here for each dataset (has to be hardcoded).\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for ds in datasets:\n",
    "    print (ds)\n",
    "    df_ds = get_df_all(ds, models_paths[ds],'Recall', 50)\n",
    "    df = df.append(df_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.variant == 'Ours'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins=4\n",
    "\n",
    "data_binned = pd.DataFrame()\n",
    "\n",
    "for ds_name in datasets:\n",
    "    \n",
    "    ds = df[(df.dataset == ds_name) ]\n",
    "    print(ds)\n",
    "    binned_ds = bin_data(ds, 'user_activity', 4, 'score')\n",
    "    print(binned_ds)\n",
    "    idx = binned_ds.groupby('bin').size().reset_index()['bin'].reset_index()\n",
    "    binned_ds = binned_ds.merge(idx, on='bin', how='inner')\n",
    "    data_binned = data_binned.append(binned_ds)\n",
    "\n",
    "data_binned.groupby(['variant', 'dataset','bin']).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_names = ['BPR', 'VAE-CF', 'Ours']\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"ticks\")\n",
    "flatui= [\"#1f78b4\",\"#33a02c\",\"#6a3d9a\",\"#fb8072\",\"#8dd3c7\", \"#fdb462\",\"#fb9a99\", \"#fb9a99\", \"#fb9a99\"]\n",
    "\n",
    "hue_kws = dict(\n",
    "               marker=[\"o\", \"^\", \"v\", \"s\", \"p\", \"*\", \".\", \".\", \".\"],\n",
    "               ls=[\"--\", \"--\", \"-.\", \"-.\", \"-\", \"-.\", \"-.\", \"-.\", \"-.\"],\n",
    "               linewidth=[5, 5, 5, 5, 7, 4, 4, 4, 4])\n",
    "\n",
    "pal = sns.mpl_palette(\"tab10\", len(plot_model_names))\n",
    "pal.reverse()\n",
    "pal[-1] = sns.color_palette(\"Blues\")[-1]\n",
    "\n",
    "g = sns.catplot(kind='bar', x=\"index\", y=\"score\", hue=\"variant\", data=data_binned,\n",
    "               col=\"dataset\", col_wrap=1, aspect=1.5,height=11, sharex=False,\n",
    "                palette=pal, hue_order=plot_model_names, legend=False, sharey=False)\n",
    "\n",
    "fontsize=40\n",
    "\n",
    "g.set_xlabels(\"User Activity Quartiles\", fontsize=fontsize)\n",
    "g.set_ylabels(\"NDCG$@50$\", fontsize=fontsize)\n",
    "\n",
    "g.axes[0].set_title(\"Dataset name\", fontsize=50)\n",
    "\n",
    "\n",
    "for it in range(0, 1):\n",
    "    g.axes[it].tick_params(direction='in', length=12, width=6, colors='k', which='major', labelsize=35, axis='y')\n",
    "    g.axes[it].tick_params(direction='in', length=6, width=3, colors='gray', which='minor', axis='y')\n",
    "        \n",
    "#g.axes[0].set_yticks(np.arange(0.0, 0.31, 0.1))\n",
    "#g.axes[0].set_yticks(np.arange(0.0, 0.31, 0.02), minor=True)\n",
    "\n",
    "g.axes[0].set_xticklabels(map(lambda x : \"Q\"+str(x), range(1, 5)), fontsize=30)\n",
    "\n",
    "g.despine(offset=10, left=True, trim=True, bottom=True)\n",
    "plt.subplots_adjust(hspace=.4)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(-0.15, 0.05, 1.0, 1.2),ncol=len(plot_model_names), fontsize=50, frameon=False)\n",
    "\n",
    "g.axes[0].tick_params(labelsize=40)\n",
    "\n",
    "# g.savefig(\"user_quartile.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
